## 网络爬虫
网络爬虫(web crawler), 以前经常称为网络蜘蛛(spider), 是按照一定的规则自动浏览万维网并获取信息的机器人程序(或叫脚本), 曾经被广泛的应用于互联网搜索引擎. 使用过互联网和浏览器的人都知道, 网页中除了提供用户阅读的文字信息之外, 还包含一些超链接. 网络爬虫系统正是通过网页中的超链接信息不断获得网络上的其他页面. 正因为如此, 网络数据采集的过程就像一个爬虫或者蜘蛛在网络上漫游, 所有才被形象的称之为网络爬虫或者网络蜘蛛.

## 爬虫的应用领域

在理想的状态下, 所有的ICP(internet Content Provider) 都应该为自己的网络提供API接口来共享它们允许其他程序获取的数据, 在这种情况下爬虫就不是必需品, 国内比较有名的电商平台(如淘宝, 京东等), 社交平台(如QQ/微博/微信等)这些网站都提供了自己的Open Api, 但是这类Open Api通常会对可以抓取的数据频率进行限制. 对于大多数的公司而言, 计时的获取行业相关数据就是企业生存的重要环节之一, 然而大部分企业在行业数据方面的匮乏是其与生俱来的短板, 合理的利用爬虫来获取数据并从中提取出有价值的信息是至关重要的. 当然爬虫还有很多重要的应用领域, 以下列举了其中一部分.

1. 搜索引擎
2. 新闻聚合
3. 社交应用
4. 舆情监控
5. 行业数据


## 合法性
1.网络爬虫领域目前还属于拓荒阶段, 虽然互联网世界已经通过自己的游戏规则建立起一定的道德规范(Robots协议, 全称是'网络爬虫排除标准'), 但在法律部分还在建立和完善中, 也就是说, 现在这个领域暂时还是灰色地带.
2. '法不禁止即为许可', 如果爬虫就像浏览器一样获取的是前端显示的数据(网页上的公开信息), 而不是网站后台的私密敏感信息, 就不太担心法律法规的约束, 因为目前大数据产业链的发展速度远远超过了法律的完善程度.
3. 在爬取网站的时候, 需要限制自己的爬虫遵守Robots协议, 同时控制网络爬虫程序的抓取数据的速度, 在使用数据的时候, 必须要尊重网站的知识产权(从Web2.0时代开始, 虽然Web上的数据很多都是由用户提供的, 但是网站平台是投入了运营成本的, 当用户在注册和发布呢日用时, 平台通常就已经获取了对数据的所有权, 使用权和分发权). 如果违反了这些规定, 在打官司的时候败诉几率就非常高.

## Robots.txt文件
网站告诉爬虫哪些能访问哪些不能访问
https://www.taobao.com/robots.txt
```
User-Agent:  Yahoo!  Slurp
Allow:  /product
Allow:  /spu
Allow:  /dianpu
Allow:  /oversea
Allow:  /list
Allow:  /ershou
Disallow:  /

User-Agent:  *
Disallow:  /
```
注意上面robots.txt第一段的最后一行, 通过设置’Disallow:/’禁止百度爬虫访问除了’Allow’规定页面外的其他所有页面. 因此当你在百度搜索’淘宝’的时候, 搜索结果下方会出现: ‘由于该网站的rebots.txt文件存在限制指令(限制搜索引擎抓取). 系统无法提供该页面的内容描述.’, 百度作为一个搜索引擎, 至少在表面上遵守了淘宝网的robots.txt协议, 所以用户不能从百度上搜索到淘宝内部的产品信息. 

## whois信息
1. 站长工具
2. whois包

## 相关工具
1. chrome devtool使用
2. postman
3. 抓包工具
 



## reference
1. [刘东灵 的CSDN 博客](https://blog.csdn.net/qq_41637554/article/details/80548242?utm_source=copy)
2. 《用python写网络爬虫》