2初步反爬策略
===
为什么反爬


## 请求头
```
import urllib2,urllib

url = 'http://www.baidu.com'
headers = {
    # User-Agent: 在设置用户标识，可以通过该键伪装成是浏览器在访问该网站。而爬虫默认的User-Agent的值是：Python-urllib/2.7。
    'User-Agent':"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0",
    # Referer: 用于标识当前是通过哪一个网址入口访问的url，服务器在读取Referer字段值得时候，会判断当前的url访问是不是通过正常的网址入口进来的。
    'Referer':"http://www.baidu.com"
}
request = urllib2.Request(url, headers=headers)

res = urllib2.urlopen(request)
```
1>服务器会判断一个频繁的请求是不是同一个User-Agent用户标识，如果是，会进行限制访问。
解决：需要随机切换User-Agent的值
2>服务器会判断User-Agent是不是以Python开头的，如果是，会限制访问。
3>服务器会判断一个频繁的请求是不是同一个IP地址发出的请求，如果是，会对IP进行限制访问。
解决：使用代理IP，随机切换IP地址。不使用真实IP来频繁的请求。请求合法开放的api。

## 设置代理
### 寻找代理
### 代码实现
```
import urllib2,urllib,random

# 设置User-Agent列表
user_agent_list = [
    "Mozilla/5.0(Macintosh;IntelMacOSX10.6;rv:2.0.1)Gecko/20100101Firefox/4.0.1",
    "Mozilla/4.0(compatible;MSIE6.0;WindowsNT5.1)",
    "Opera/9.80(WindowsNT6.1;U;en)Presto/2.8.131Version/11.11",
    "Mozilla/5.0(Macintosh;IntelMacOSX10_7_0)AppleWebKit/535.11(KHTML,likeGecko)Chrome/17.0.963.56Safari/535.11",
    "Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1)",
    "Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;Trident/4.0;SE2.XMetaSr1.0;SE2.XMetaSr1.0;.NETCLR2.0.50727;SE2.XMetaSr1.0)"
]

headers = {
    # 从列表中随机取出一个浏览器标识
    'User-Agent': random.choice(user_agent_list)
}

# 设置IP列表
ip_list = [
    "117.68.253.144:808",
    "112.114.93.50:8118",
]

proxies = {
    'http':random.choice(ip_list)
}

request = urllib2.Request('http://www.baidu.com', headers=headers)

# 创建IP代理对象
proxy_handler = urllib2.ProxyHandler(proxies)
# 根据代理对象，创建用于发送请求的opener对象
opener = urllib2.build_opener(proxy_handler)
# 再使用opener对象发出请求
res = opener.open(request)

print(res.read())
```

## 其它先不讲
伪造cookie
伪造session